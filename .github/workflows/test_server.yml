name: Test LlamaEdge-Nexus

on:
  push:
    branches:
      - dev
      - main
      - release-*
      - feat-*
      - ci-*
      - refactor-*
      - fix-*
      - test-*
    paths:
      - '.github/workflows/test_server.yml'
      - '**/Cargo.toml'
      - '**/*.rs'
      - '**/*.sh'
      - '**/.cargo/config.toml'
      - 'tests/*.hurl'
  pull_request:
    branches:
      - dev
      - main
    types: [opened, synchronize, reopened]
    paths:
      - '.github/workflows/**'
      - '**/Cargo.toml'
      - '**/*.rs'
      - '**/*.sh'
      - 'tests/*.hurl'

jobs:
  test-server:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        wasmedge_version: [0.14.1]
        llama_api_server_version: [0.14.4]
        whisper_api_server_version: [0.4.2]

    steps:
      - name: Clone project
        id: checkout
        uses: actions/checkout@v3

      - name: Install Rust-nightly
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: nightly
          target: wasm32-wasip1
          components: rustfmt, clippy

      - name: Install Rust-stable
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          target: wasm32-wasip1

      - name: Install Hurl
        run: |
          curl --location --remote-name https://github.com/Orange-OpenSource/hurl/releases/download/5.0.1/hurl_5.0.1_amd64.deb
          sudo apt update && sudo apt install ./hurl_5.0.1_amd64.deb

      - name: Install WasmEdge
        run: |
          curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v ${{ matrix.wasmedge_version }}
          ls -al $HOME/.wasmedge/bin

      - name: Build llama-nexus
        env:
          RUSTFLAGS: "--cfg wasmedge --cfg tokio_unstable"
        run: |
          cargo build --release
          cp target/wasm32-wasip1/release/llama-nexus.wasm .
          ls -al

      - name: Build llama-api-server.wasm
        env:
          RUSTFLAGS: "--cfg wasmedge --cfg tokio_unstable"
        run: |
          git clone -b refactor-update-serverinfo https://github.com/LlamaEdge/LlamaEdge.git
          cd LlamaEdge
          cargo build --release
          cp target/wasm32-wasip1/release/llama-api-server.wasm ../llama-api-server.wasm
          cd -

      - name: Deploy whisper plugin
        run: |
          curl -LO https://github.com/WasmEdge/WasmEdge/releases/download/${{ matrix.wasmedge_version }}/WasmEdge-plugin-wasi_nn-whisper-${{ matrix.wasmedge_version }}-ubuntu20.04_x86_64.tar.gz
          tar -xzvf WasmEdge-plugin-wasi_nn-whisper-${{ matrix.wasmedge_version }}-ubuntu20.04_x86_64.tar.gz
          mkdir -p $HOME/whisper/plugin/
          ls -al

          mv libwasmedgePluginWasiNN.so $HOME/whisper/plugin/
          ls -al $HOME/whisper/plugin/

      - name: Start LlamaEdge-Nexus
        run: |
          nohup $HOME/.wasmedge/bin/wasmedge --dir .:. llama-nexus.wasm > ./llama-nexus.log 2>&1 &
          sleep 20
          cat llama-nexus.log

      - name: Print LlamaEdge-Nexus logs on failure
        if: failure()
        run: |
          echo "LlamaEdge-Nexus logs:"
          cat  llama-nexus.log

      - name: Start chat server
        run: |
          curl -LO https://huggingface.co/second-state/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf
          nohup $HOME/.wasmedge/bin/wasmedge --dir .:. --nn-preload default:GGML:AUTO:Llama-3.2-1B-Instruct-Q4_0.gguf llama-api-server.wasm --model-name Llama-3.2-1B --prompt-template llama-3-chat --ctx-size 4096 --port 10010 > ./chat-server.log 2>&1 &
          sleep 15
          cat chat-server.log

      - name: Register chat server
        if: success()
        run: |
          curl --location 'http://localhost:9068/admin/servers/register' --header 'Content-Type: application/json' --data '{"url": "http://localhost:10010","kind": "chat"}'
          sleep 5

      - name: Print chat server logs on failure
        if: failure()
        run: |
          echo "chat server logs:"
          cat chat-server.log

      - name: Print llama-nexus logs on failure
        if: failure()
        run: |
          echo "llama-nexus logs:"
          cat llama-nexus.log

      - name: Start embedding server
        run: |
          curl -LO https://huggingface.co/second-state/nomic-embed-text-v1.5-GGUF/resolve/main/nomic-embed-text-v1.5-Q4_0.gguf
          nohup $HOME/.wasmedge/bin/wasmedge --dir .:. --nn-preload default:GGML:AUTO:nomic-embed-text-v1.5-Q4_0.gguf llama-api-server.wasm --model-name nomic-embed-text-v1.5 --prompt-template embedding --ctx-size 768 --port 10011 > ./embedding-server.log 2>&1 &
          sleep 15
          cat embedding-server.log

      - name: Register embedding server
        if: success()
        run: |
          curl --location 'http://localhost:9068/admin/servers/register' --header 'Content-Type: application/json' --data '{"url": "http://localhost:10011","kind": "embeddings"}'
          sleep 5

      - name: Print embedding server logs on failure
        if: failure()
        run: |
          echo "embedding server logs:"
          cat embedding-server.log

      - name: Print llama-nexus logs on failure
        if: failure()
        run: |
          echo "llama-nexus logs:"
          cat llama-nexus.log

      - name: Start whisper server
        run: |
          curl -LO https://github.com/LlamaEdge/whisper-api-server/releases/download/${{ matrix.whisper_api_server_version }}/whisper-api-server.wasm
          curl -LO https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v2-q5_0.bin
          nohup env WASMEDGE_PLUGIN_PATH=$HOME/whisper/plugin $HOME/.wasmedge/bin/wasmedge --dir .:. whisper-api-server.wasm -m ggml-large-v2-q5_0.bin --port 10012 > ./whisper-server.log 2>&1 &
          sleep 15
          cat whisper-server.log

      - name: Register whisper server
        if: success()
        run: |
          curl --location 'http://localhost:9068/admin/servers/register' --header 'Content-Type: application/json' --data '{"url": "http://localhost:10012","kind": "translate,transcribe"}'
          sleep 5

      - name: Print whisper server logs on failure
        if: failure()
        run: |
          echo "whisper server logs:"
          cat whisper-server.log

      - name: Print llama-nexus logs on failure
        if: failure()
        run: |
          echo "llama-nexus logs:"
          cat llama-nexus.log

      - name: Test chat server
        if: success()
        run: |
          hurl --test --jobs 1 ./tests/test_chat.hurl

      - name: Print chat server logs on failure
        if: failure()
        run: |
          echo "chat server logs:"
          cat chat-server.log

      - name: Test embedding server
        if: success()
        run: |
          hurl --test --jobs 1 ./tests/test_embeddings.hurl

      - name: Print embedding server logs on failure
        if: failure()
        run: |
          echo "embedding server logs:"
          cat embedding-server.log

      - name: Print llama-nexus logs on failure
        if: failure()
        run: |
          echo "llama-nexus logs:"
          cat llama-nexus.log

      # - name: Run test_transcribe.hurl
      #   if: always()
      #   run: |
      #     cp ./tests/assets/test.wav ./tests/test.wav
      #     hurl --test --jobs 1 ./tests/test_transcribe.hurl

      # - name: Print whisper logs on failure
      #   if: always()
      #   run: |
      #     echo "Whisper server logs:"
      #     cat ./start-whisper.log

      # - name: Run test_translate.hurl
      #   if: always()
      #   run: |
      #     cp ./tests/assets/test_cn.wav ./tests/test_cn.wav
      #     hurl --test --jobs 1 ./tests/test_translate.hurl

      # - name: Print whisper logs on failure
      #   if: always()
      #   run: |
      #     echo "Whisper server logs:"
      #     cat ./start-whisper.log

      - name: Stop LlamaEdge-Nexus
        run: |
          pkill -f llama-nexus

      - name: Stop llama-api-server and whisper-api-server
        run: |
          pkill -f wasmedge
