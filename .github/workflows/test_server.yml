name: Test Llama-Gateway

on:
  push:
    branches:
      - dev
      - main
      - release-*
      - feat-*
      - ci-*
      - refactor-*
      - fix-*
      - test-*
    paths:
      - '.github/workflows/test_server.yml'
      - '**/Cargo.toml'
      - '**/*.rs'
      - '**/*.sh'
      - '**/.cargo/config.toml'
      - 'tests/*.hurl'
  pull_request:
    branches:
      - dev
      - main
    types: [opened, synchronize, reopened]
    paths:
      - '.github/workflows/**'
      - '**/Cargo.toml'
      - '**/*.rs'
      - '**/*.sh'
      - 'tests/*.hurl'

jobs:
  test-server:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        wasmedge_version: [0.14.1]
        llama_api_server_version: [0.14.4]
        whisper_api_server_version: [0.2.2]

    steps:
      - name: Clone project
        id: checkout
        uses: actions/checkout@v3

      - name: Install Rust-nightly
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: nightly
          target: wasm32-wasip1
          components: rustfmt, clippy

      - name: Install Rust-stable
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          target: wasm32-wasip1

      - name: Install WasmEdge
        run: |
          curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v ${{ matrix.wasmedge_version }}
          ls -al $HOME/.wasmedge/bin

      - name: Install Hurl
        run: |
          curl --location --remote-name https://github.com/Orange-OpenSource/hurl/releases/download/5.0.1/hurl_5.0.1_amd64.deb
          sudo apt update && sudo apt install ./hurl_5.0.1_amd64.deb

      - name: Download api-server
        run: |
          curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/download/${{ matrix.llama_api_server_version }}/llama-api-server.wasm
          curl -LO https://github.com/LlamaEdge/whisper-api-server/releases/download/${{ matrix.whisper_api_server_version }}/whisper-api-server.wasm

      - name: Download model used by llama-api-server
        run: |
          curl -LO https://huggingface.co/second-state/Qwen2-1.5B-Instruct-GGUF/resolve/main/Qwen2-1.5B-Instruct-Q3_K_M.gguf
          ls -al

      - name: Download model and audio files
        run: |
          curl -LO https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-medium.bin
          ls -al

    #   - name: Start llama-api-server at 10086 port
    #     run: |
    #       nohup $HOME/.wasmedge/bin/wasmedge --dir .:. --nn-preload default:GGML:AUTO:Qwen2-1.5B-Instruct-Q3_K_M.gguf llama-api-server.wasm --model-name Qwen2-1.5B-Instruct --prompt-template chatml --ctx-size 4096 --socket-addr 0.0.0.0:10086 > ./start-llama.log 2>&1 &
    #       sleep 5
    #       cat start-llama.log

    #   - name: Start whisper-api-server at 10087 port
    #     run: |
    #       nohup $HOME/.wasmedge/bin/wasmedge --dir .:. whisper-api-server.wasm -m ggml-medium.bin --socket-addr 0.0.0.0:10087 > ./start-llamaedge.log 2>&1 &
    #       sleep 5
    #       cat start-llamaedge.log

